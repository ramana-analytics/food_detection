{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3ece",
   "metadata": {},
   "source": [
    "# Model Version 1 (3 layer sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5835853",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67e72e",
   "metadata": {},
   "source": [
    "Set our data directory and verify image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('similar foods small sample')\n",
    "\n",
    "image_count = (len(list(data_dir.glob('*/*.png'))) + \n",
    "               len(list(data_dir.glob('*/*.jpg'))) + \n",
    "               len(list(data_dir.glob('*/*.jpeg'))))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fae91",
   "metadata": {},
   "source": [
    "Define parameters, convert our directory of images into a tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # accounting for 1/very few images per class\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571ef5a",
   "metadata": {},
   "source": [
    "Set up variables for building our training and validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076621b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ds.class_names\n",
    "num_classes = len(class_names)\n",
    "validation_split = .2\n",
    "\n",
    "# initializing val_ds and train_ds as \"empty\" arrays (set to ds to preserve dimensions, filtered all content out)\n",
    "val_ds = ds.filter(lambda x, l: tf.math.equal(l[0], num_classes + 1))\n",
    "train_ds = ds.filter(lambda x, l: tf.math.equal(l[0], num_classes + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498f200",
   "metadata": {},
   "source": [
    "We want validation representation from each class. however, its very small dataset so we can't rely on random validation sampling to assign validation images evenly from each class, so we gaurantee that there is at least one validation image for each class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_classes):\n",
    "    filtered_ds = ds.filter(lambda x, l: tf.math.equal(l[0], i))\n",
    "  \n",
    "    fds_list = list(filtered_ds)\n",
    "    val_count = len(fds_list) // (1/validation_split)\n",
    "\n",
    "    # gaurantee that there is at least one validation image for each class\n",
    "    if val_count == 0:\n",
    "      val_count = 1\n",
    "\n",
    "    if len(fds_list) > 0:\n",
    "      val_ds = val_ds.concatenate(filtered_ds.take(val_count))\n",
    "      train_ds = train_ds.concatenate(filtered_ds.skip(val_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25cca1",
   "metadata": {},
   "source": [
    "I'm pretty sure this stuff only matters for much larger datasets but still good practice to include. Note: we shuffle training data but not validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af295c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c6f9f",
   "metadata": {},
   "source": [
    "Define and build model (note data_augmentation step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbddcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Rescaling(1./255),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  #layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9506d",
   "metadata": {},
   "source": [
    "Run model (seems to level out around 50 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7f102",
   "metadata": {},
   "source": [
    "Generate accuracy and loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee2e36",
   "metadata": {},
   "source": [
    "Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = pathlib.Path('test images/shredded wheat.jpeg')\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    test_img_path, target_size=(img_height, img_width)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
